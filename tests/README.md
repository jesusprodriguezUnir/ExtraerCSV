# Tests y ValidaciÃ³n de Calidad

Este directorio contiene tests automatizados para el extractor de datos JSON.

## ğŸ§ª Ejecutar tests

### Instalar dependencias de testing

```bash
pip install -r requirements-dev.txt
```

### Ejecutar todos los tests

```bash
pytest
```

### Ejecutar tests con cobertura

```bash
pytest --cov=. --cov-report=html
```

Esto generarÃ¡ un reporte HTML en `htmlcov/index.html`.

### Ejecutar tests especÃ­ficos

```bash
# Un archivo especÃ­fico
pytest tests/test_data_processor.py

# Una clase especÃ­fica
pytest tests/test_data_processor.py::TestNormalizarJson

# Un test especÃ­fico
pytest tests/test_data_processor.py::TestNormalizarJson::test_extrae_json_correctamente

# Con verbose
pytest -v

# Con output de print
pytest -s
```

### Ejecutar tests en modo watch (desarrollo)

```bash
pytest-watch
```

## ğŸ“ Estructura de tests

```
tests/
â”œâ”€â”€ __init__.py                      # InicializaciÃ³n del paquete
â”œâ”€â”€ test_data_processor.py           # Tests para data_processor.py
â”œâ”€â”€ test_config.py                   # Tests para config.py
â”œâ”€â”€ test_elasticsearch_client.py     # Tests para elasticsearch_client.py (con mocks)
â””â”€â”€ README.md                        # Esta documentaciÃ³n
```

## ğŸ§© Tipos de tests

### Unit Tests
Tests aislados de funciones individuales sin dependencias externas.

**Ejemplo:**
- `test_normalizar_json` - Prueba la extracciÃ³n de JSON
- `test_extraer_valores_no_nulos` - Prueba el filtrado de valores

### Integration Tests
Tests que verifican la interacciÃ³n entre mÃºltiples componentes.

**Ejemplo:**
- `test_flujo_completo_csv_simulado` - Simula el flujo completo de procesamiento
- `test_procesa_registros_correctamente` - Verifica el pipeline completo

### Mocked Tests
Tests con mocks para componentes externos (Elasticsearch).

**Ejemplo:**
- `test_conexion_exitosa` - Mock de conexiÃ³n a Elasticsearch
- `test_busca_logs_correctamente` - Mock de bÃºsquedas con scan API

## ğŸ¯ Cobertura de tests

Los tests cubren:

âœ… **data_processor.py**
- ExtracciÃ³n de JSON de mensajes
- Filtrado de valores nulos
- Procesamiento de registros iterables
- EliminaciÃ³n de duplicados
- Casos edge: mensajes vacÃ­os, JSON malformado

âœ… **config.py**
- Carga de variables de entorno
- ValidaciÃ³n de configuraciÃ³n
- Valores por defecto
- ConversiÃ³n de tipos
- Manejo de errores

âœ… **elasticsearch_client.py**
- InicializaciÃ³n del cliente (mock)
- Test de conexiÃ³n
- Listado de Ã­ndices
- BÃºsqueda con Scroll API
- Descarga a CSV
- Manejo de errores (auth, SSL, Ã­ndices no encontrados)

## ğŸ”§ Escribir nuevos tests

### Template bÃ¡sico

```python
import pytest

def test_mi_nueva_funcionalidad():
    """Test: DescripciÃ³n de quÃ© se estÃ¡ probando"""
    # Arrange (preparar)
    input_data = "datos de prueba"
    
    # Act (ejecutar)
    result = mi_funcion(input_data)
    
    # Assert (verificar)
    assert result == "resultado esperado"
```

### Usando fixtures

```python
@pytest.fixture
def datos_prueba():
    """Fixture que proporciona datos de prueba reutilizables"""
    return {
        "field": "test",
        "value": 123
    }

def test_con_fixture(datos_prueba):
    """Test que usa el fixture"""
    assert datos_prueba["field"] == "test"
```

### Usando mocks

```python
from unittest.mock import Mock, patch

def test_con_mock():
    """Test usando mock para Elasticsearch"""
    with patch('elasticsearch_client.Elasticsearch') as MockES:
        mock_instance = MockES.return_value
        mock_instance.ping.return_value = True
        
        # Tu cÃ³digo de test aquÃ­
        assert mock_instance.ping() is True
```

### Usando archivos temporales

```python
def test_con_archivo_temporal(tmp_path):
    """Test que crea archivos temporales"""
    archivo = tmp_path / "test.json"
    archivo.write_text('{"test": true}')
    
    assert archivo.exists()
    # El archivo se limpia automÃ¡ticamente despuÃ©s del test
```

## ğŸ“Š Mejores prÃ¡cticas

### âœ… DO (Hacer)

- Escribe tests descriptivos con nombres claros
- Usa fixtures para datos reutilizables
- Un assert por concepto (pero pueden ser mÃºltiples asserts relacionados)
- Mockea dependencias externas (APIs, bases de datos)
- Prueba casos normales y edge cases
- MantÃ©n los tests independientes entre sÃ­

### âŒ DON'T (No hacer)

- No dependas de orden de ejecuciÃ³n de tests
- No uses sleeps o esperas innecesarias
- No compartas estado entre tests
- No pruebes implementaciÃ³n interna, prueba comportamiento
- No skipees tests sin razÃ³n documentada

## ğŸ› Debugging tests

### Test fallido - ver output completo

```bash
pytest -vv --tb=long
```

### Entrar en debugger cuando falla

```bash
pytest --pdb
```

### Ver print statements

```bash
pytest -s
```

### Ejecutar solo tests marcados

```python
@pytest.mark.slow
def test_operacion_lenta():
    pass

# Ejecutar solo tests lentos
pytest -m slow

# Ejecutar todos excepto los lentos
pytest -m "not slow"
```

## ğŸ“ˆ CI/CD Integration

### GitHub Actions example

```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.11
      - run: pip install -r requirements.txt -r requirements-dev.txt
      - run: pytest --cov=. --cov-report=xml
      - uses: codecov/codecov-action@v2
```

## ğŸ“ Recursos

- [Pytest Documentation](https://docs.pytest.org/)
- [Unittest.mock Documentation](https://docs.python.org/3/library/unittest.mock.html)
- [Testing Best Practices](https://docs.pytest.org/en/stable/goodpractices.html)

## ğŸ“ Ayuda

Si encuentras problemas con los tests:

1. Verifica que instalaste `requirements-dev.txt`
2. Ejecuta `pytest --version` para confirmar instalaciÃ³n
3. Limpia cache: `pytest --cache-clear`
4. Revisa los logs con `-vv --tb=long`
